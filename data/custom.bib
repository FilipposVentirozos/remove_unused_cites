% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{Namee2022,
  author = {Khanista Namee and Jantima Polpinij and Bancha Luaphol},
  title = {A Hybrid Approach for Aspect-based Sentiment Analysis: A Case Study of Hotel Reviews},
  journal = {Current Applied Science and Technology},
  volume = {23},
  number = {2},
  year = {2023},
  pages = {Published: Aug 15, 2022},
  doi = {10.55003/cast.2022.02.23.008},
  url = {https://li01.tci-thaijo.org/index.php/cast/article/view/253287},
  keywords = {sentiment analysis, aspect level, Word2Vec, support vector machines, convolutional neural network, BM25},
  abstract = {This study presents a method of aspect-based sentiment analysis for customer reviews related to hotels. The considered hotel aspects are staff attentiveness, room cleanliness, value for money and convenience of location. The proposed method consists of two main components. The first component is used to assemble relevant sentences for each hotel aspect into relevant clusters of hotel aspects using BM25. We developed a corpus of keywords called the Keywords of Hotel Aspect (KoHA) Corpus, and the keywords of each aspect were used as queries to assemble relevant sentences of each hotel aspect into relevant clusters. Finally, customer review sentences in each cluster were classified into positive and negative classes using sentiment classifiers. Two algorithms, Support Vector Machines (SVM) with a linear and a RBF kernel, and Convolutional Neural Network (CNN) were applied to develop the sentiment classifier models. The model based on SVM with a linear kernel returned better results than other models with an AUC score of 0.87. Therefore, this model was chosen for the sentiment classification stage. The proposed method was evaluated using recall, precision and F1 with satisfactory results at 0.85, 0.87 and 0.86, respectively. Our proposed method provided an overview of customer feelings based on score, and also provided reasons why customers liked or disliked each aspect of the hotel. The best model from the proposed method was used to compare with a state-of-the-art model. The results show that our method increased recall, precision, and F1 scores by 2.44\%, 2.50\% and 1.84\%, respectively.}
}

@INPROCEEDINGS{10099815,
  author={Kit, Brentton Wong Swee and Joseph, Minnu Helen},
  booktitle={2023 15th International Conference on Developments in eSystems Engineering (DeSE)}, 
  title={Aspect-Based Sentiment Analysis on Movie Reviews}, 
  year={2023},
  volume={},
  number={},
  pages={237-243},
  keywords={Industries;Sentiment analysis;Machine learning;Companies;Predictive models;Motion pictures;Decision trees;Sentiment Analysis;Aspect-Based;Text Analysis;Machine Learning;Polarity;Movie;Reviews},
  doi={10.1109/DeSE58274.2023.10099815}}

@article{Chu2022,
  author = {M. Chu and Y. Chen and L. Yang and J. Wang},
  title = {Language interpretation in travel guidance platform: Text mining and sentiment analysis of TripAdvisor reviews},
  journal = {Frontiers in Psychology},
  year = {2022},
  volume = {13},
  doi = {10.3389/fpsyg.2022.1029945},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2022.1029945/full}
}


@ARTICLE{10151883,
  author={Yu, Yang and Dinh, Duy-Tai and Nguyen, Ba-Hung and Yu, Fangyu and Huynh, Van-Nam},
  journal={IEEE Access}, 
  title={Mining Insights From Esports Game Reviews With an Aspect-Based Sentiment Analysis Framework}, 
  year={2023},
  volume={11},
  number={},
  pages={61161-61172},
  keywords={Games;Sentiment analysis;Task analysis;Transformers;Resource management;Bit error rate;Analytical models;Esports;topic modeling;prevalence analysis;sentiment analysis;steam},
  doi={10.1109/ACCESS.2023.3285864}}


@ARTICLE{9996141,
  author={Zhang, Wenxuan and Li, Xin and Deng, Yang and Bing, Lidong and Lam, Wai},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={A Survey on Aspect-Based Sentiment Analysis: Tasks, Methods, and Challenges}, 
  year={2023},
  volume={35},
  keywords={Task analysis;Sentiment analysis;Compounds;Data mining;Taxonomy;Analytical models;Systematics;Aspect-based sentiment analysis;opinion mining;pre-trained language models;sentiment analysis},
  doi={10.1109/TKDE.2022.3230975}}


@article{Wan_Yang_Du_Liu_Qi_Pan_2020, title={Target-Aspect-Sentiment Joint Detection for Aspect-Based Sentiment Analysis}, volume={34}, url={https://ojs.aaai.org/index.php/AAAI/article/view/6447}, DOI={10.1609/aaai.v34i05.6447}, abstractNote={&lt;p&gt;&lt;em&gt;Aspect-based sentiment analysis&lt;/em&gt; (ABSA) aims to detect the targets (which are composed by continuous words), aspects and sentiment polarities in text. Published datasets from SemEval-2015 and SemEval-2016 reveal that a sentiment polarity depends on both the target and the aspect. However, most of the existing methods consider predicting sentiment polarities from either targets or aspects but not from both, thus they easily make wrong predictions on sentiment polarities. In particular, where the target is implicit, &lt;em&gt;i.e.&lt;/em&gt;, it does not appear in the given text, the methods predicting sentiment polarities from targets do not work. To tackle these limitations in ABSA, this paper proposes a novel method for target-aspect-sentiment joint detection. It relies on a pre-trained language model and can capture the dependence on both targets and aspects for sentiment prediction. Experimental results on the SemEval-2015 and SemEval-2016 restaurant datasets show that the proposed method achieves a high performance in detecting target-aspect-sentiment triples even for the implicit target cases; moreover, it even outperforms the state-of-the-art methods for those subtasks of target-aspect-sentiment detection that they are competent to.&lt;/p&gt;}, number={05}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Wan, Hai and Yang, Yufei and Du, Jianfeng and Liu, Yanan and Qi, Kunxun and Pan, Jeff Z.}, year={2020}, month={Apr.}, pages={9122-9129} }


@article{t5,
author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
title = {Exploring the limits of transfer learning with a unified text-to-text transformer},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {140},
numpages = {67},
keywords = {transfer learning, natural language processing, multi-task learning, attention based models, deep learning}
}


@INPROCEEDINGS{10447700,
  author={Zhang, Wenyuan and Zhang, Xinghua and Cui, Shiyao and Huang, Kun and Wang, Xuebin and Liu, Tingwen},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Adaptive Data Augmentation for Aspect Sentiment Quad Prediction}, 
  year={2024},
  volume={},
  number={},
  pages={11176-11180},
  keywords={Adaptation models;Sentiment analysis;Tail;Signal processing;Data augmentation;Generators;Cognition;Aspect sentiment quad prediction;Data imbalance;Data augmentation;Generative framework},
  doi={10.1109/ICASSP48485.2024.10447700}}

@INPROCEEDINGS{10191634,
  author={Yu, Yongxin and Zhao, Minyi and Zhou, Shuigeng},
  booktitle={2023 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Boosting Aspect Sentiment Quad Prediction by Data Augmentation and Self-Training}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  keywords={Sentiment analysis;Semantics;Neural networks;Training data;Predictive models;Data augmentation;Data models;Aspect-based sentiment analysis;Aspect sentiment quad prediction;Data augmentation;Self-training},
  doi={10.1109/IJCNN54540.2023.10191634}}

@INPROCEEDINGS{10394369,
  author={Lil, Zhijun and Yang, Zhenyu and Li, Xiaoyang and Li, Yiwen},
  booktitle={2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title={Two-Stage Aspect Sentiment Quadruple Prediction Based on MRC and Text Generation}, 
  year={2023},
  volume={},
  number={},
  pages={2118-2125},
  keywords={Technological innovation;Correlation;Semantics;Natural languages;Tagging;Generators;Task analysis},
  doi={10.1109/SMC53992.2023.10394369}}

@inproceedings{chebolu-etal-2024-oats-challenge,
    title = "{OATS}: A Challenge Dataset for Opinion Aspect Target Sentiment Joint Detection for Aspect-Based Sentiment Analysis",
    author = "Chebolu, Siva Uday Sampreeth  and
      Dernoncourt, Franck  and
      Lipka, Nedim  and
      Solorio, Thamar",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.1080",
    pages = "12336--12347",
    abstract = "Aspect-based sentiment analysis (ABSA) delves into understanding sentiments specific to distinct elements within a user-generated review. It aims to analyze user-generated reviews to determine a) the target entity being reviewed, b) the high-level aspect to which it belongs, c) the sentiment words used to express the opinion, and d) the sentiment expressed toward the targets and the aspects. While various benchmark datasets have fostered advancements in ABSA, they often come with domain limitations and data granularity challenges. Addressing these, we introduce the OATS dataset, which encompasses three fresh domains and consists of 27,470 sentence-level quadruples and 17,092 review-level tuples. Our initiative seeks to bridge specific observed gaps in existing datasets: the recurrent focus on familiar domains like restaurants and laptops, limited data for intricate quadruple extraction tasks, and an occasional oversight of the synergy between sentence and review-level sentiments. Moreover, to elucidate OATS{'}s potential and shed light on various ABSA subtasks that OATS can solve, we conducted experiments, establishing initial baselines. We hope the OATS dataset augments current resources, paving the way for an encompassing exploration of ABSA (https://github.com/RiTUAL-UH/OATS-ABSA).",
}

@INPROCEEDINGS{10499502,
  author={Wang, Yajing and Luo, Zongwei},
  booktitle={2023 International Conference on High Performance Big Data and Intelligent Systems (HDIS)}, 
  title={Enhance Multi-Domain Sentiment Analysis of Review Texts Through Prompting Strategies}, 
  year={2023},
  volume={},
  number={},
  pages={1-7},
  keywords={Sentiment analysis;Analytical models;Costs;Reviews;Big Data;Data models;Task analysis;large language models;prompting strategies;sentiment analysis;implicit sentiment analysis},
  doi={10.1109/HDIS60872.2023.10499502}}


@article{Dong2023ASF,
  title={A Survey for In-context Learning},
  author={Qingxiu Dong and Lei Li and Damai Dai and Ce Zheng and Zhiyong Wu and Baobao Chang and Xu Sun and Jingjing Xu and Lei Li and Zhifang Sui},
  journal={ArXiv},
  year={2023},
  volume={abs/2301.00234},
  url={https://api.semanticscholar.org/CorpusID:263886074}
}

@article{Xu2023TheLO,
  title={The Limits of ChatGPT in Extracting Aspect-Category-Opinion-Sentiment Quadruples: A Comparative Analysis},
  author={Xiancai Xu and Jia-Dong Zhang and Rongchang Xiao and Lei Xiong},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.06502},
  url={https://api.semanticscholar.org/CorpusID:263830120}
}

@article{Zhang2023SentimentAI,
  title={Sentiment Analysis in the Era of Large Language Models: A Reality Check},
  author={Wenxuan Zhang and Yue Deng and Bing-Quan Liu and Sinno Jialin Pan and Lidong Bing},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.15005},
  url={https://api.semanticscholar.org/CorpusID:258866189}
}

@misc{openai2024gpt4,
      title={{GPT}-4 Technical Report}, 
      author={OpenAI},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      abstract={and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph}
}

@misc{geminiteam2024gemini,
      title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context}, 
      author={Team Gemini},
      year={2024},
      eprint={2403.05530},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
    abstract={ and Machel Reid and Nikolay Savinov and Denis Teplyashin and Dmitry and Lepikhin and Timothy Lillicrap and Jean-baptiste Alayrac and Radu Soricut and Angeliki Lazaridou and Orhan Firat and Julian Schrittwieser and Ioannis Antonoglou and Rohan Anil and Sebastian Borgeaud and Andrew Dai and Katie Millican and Ethan Dyer and Mia Glaese and Thibault Sottiaux and Benjamin Lee and Fabio Viola and Malcolm Reynolds and Yuanzhong Xu and James Molloy and Jilin Chen and Michael Isard and Paul Barham and Tom Hennigan and Ross McIlroy and Melvin Johnson and Johan Schalkwyk and Eli Collins and Eliza Rutherford and Erica Moreira and Kareem Ayoub and Megha Goel and Clemens Meyer and Gregory Thornton and Zhen Yang and Henryk Michalewski and Zaheer Abbas and Nathan Schucher and Ankesh Anand and Richard Ives and James Keeling and Karel Lenc and Salem Haykal and Siamak Shakeri and Pranav Shyam and Aakanksha Chowdhery and Roman Ring and Stephen Spencer and Eren Sezener and Luke Vilnis and Oscar Chang and Nobuyuki Morioka and George Tucker and Ce Zheng and Oliver Woodman and Nithya Attaluri and Tomas Kocisky and Evgenii Eltyshev and Xi Chen and Timothy Chung and Vittorio Selo and Siddhartha Brahma and Petko Georgiev and Ambrose Slone and Zhenkai Zhu and James Lottes and Siyuan Qiao and Ben Caine and Sebastian Riedel and Alex Tomala and Martin Chadwick and Juliette Love and Peter Choy and Sid Mittal and Neil Houlsby and Yunhao Tang and Matthew Lamm and Libin Bai and Qiao Zhang and Luheng He and Yong Cheng and Peter Humphreys and Yujia Li and Sergey Brin and Albin Cassirer and Yingjie Miao and Lukas Zilka and Taylor Tobin and Kelvin Xu and Lev Proleev and Daniel Sohn and Alberto Magni and Lisa Anne Hendricks and Isabel Gao and Santiago Ontanon and Oskar Bunyan and Nathan Byrd and Abhanshu Sharma and Biao Zhang and Mario Pinto and Rishika Sinha and Harsh Mehta and Dawei Jia and Sergi Caelles and Albert Webson and Alex Morris and Becca Roelofs and Yifan Ding and Robin Strudel and Xuehan Xiong and Marvin Ritter and Mostafa Dehghani and Rahma Chaabouni and Abhijit Karmarkar and Guangda Lai and Fabian Mentzer and Bibo Xu and YaGuang Li and Yujing Zhang and Tom Le Paine and Alex Goldin and Behnam Neyshabur and Kate Baumli and Anselm Levskaya and Michael Laskin and Wenhao Jia and Jack W. Rae and Kefan Xiao and Antoine He and Skye Giordano and Lakshman Yagati and Jean-Baptiste Lespiau and Paul Natsev and Sanjay Ganapathy and Fangyu Liu and Danilo Martins and Nanxin Chen and Yunhan Xu and Megan Barnes and Rhys May and Arpi Vezer and Junhyuk Oh and Ken Franko and Sophie Bridgers and Ruizhe Zhao and Boxi Wu and Basil Mustafa and Sean Sechrist and Emilio Parisotto and Thanumalayan Sankaranarayana Pillai and Chris Larkin and Chenjie Gu and Christina Sorokin and Maxim Krikun and Alexey Guseynov and Jessica Landon and Romina Datta and Alexander Pritzel and Phoebe Thacker and Fan Yang and Kevin Hui and Anja Hauth and Chih-Kuan Yeh and David Barker and Justin Mao-Jones and Sophia Austin and Hannah Sheahan and Parker Schuh and James Svensson and Rohan Jain and Vinay Ramasesh and Anton Briukhov and Da-Woon Chung and Tamara von Glehn and Christina Butterfield and Priya Jhakra and Matthew Wiethoff and Justin Frye and Jordan Grimstad and Beer Changpinyo and Charline Le Lan and Anna Bortsova and Yonghui Wu and Paul Voigtlaender and Tara Sainath and Shane Gu and Charlotte Smith and Will Hawkins and Kris Cao and James Besley and Srivatsan Srinivasan and Mark Omernick and Colin Gaffney and Gabriela Surita and Ryan Burnell and Bogdan Damoc and Junwhan Ahn and Andrew Brock and Mantas Pajarskas and Anastasia Petrushkina and Seb Noury and Lorenzo Blanco and Kevin Swersky and Arun Ahuja and Thi Avrahami and Vedant Misra and Raoul de Liedekerke and Mariko Iinuma and Alex Polozov and Sarah York and George van den Driessche and Paul Michel and Justin Chiu and Rory Blevins and Zach Gleicher and Adrià Recasens and Alban Rrustemi and Elena Gribovskaya and Aurko Roy and Wiktor Gworek and Sébastien M. R. Arnold and Lisa Lee and James Lee-Thorp and Marcello Maggioni and Enrique Piqueras and Kartikeya Badola and Sharad Vikram and Lucas Gonzalez and Anirudh Baddepudi and Evan Senter and Jacob Devlin and James Qin and Michael Azzam and Maja Trebacz and Martin Polacek and Kashyap Krishnakumar and Shuo-yiin Chang and Matthew Tung and Ivo Penchev and Rishabh Joshi and Kate Olszewska and Carrie Muir and Mateo Wirth and Ale Jakse Hartman and Josh Newlan and Sheleem Kashem and Vijay Bolina and Elahe Dabir and Joost van Amersfoort and Zafarali Ahmed and James Cobon-Kerr and Aishwarya Kamath and Arnar Mar Hrafnkelsson and Le Hou and Ian Mackinnon and Alexandre Frechette and Eric Noland and Xiance Si and Emanuel Taropa and Dong Li and Phil Crone and Anmol Gulati and Sébastien Cevey and Jonas Adler and Ada Ma and David Silver and Simon Tokumine and Richard Powell and Stephan Lee and Kiran Vodrahalli and Samer Hassan and Diana Mincu and Antoine Yang and Nir Levine and Jenny Brennan and Mingqiu Wang and Sarah Hodkinson and Jeffrey Zhao and Josh Lipschultz and Aedan Pope and Michael B. Chang and Cheng Li and Laurent El Shafey and Michela Paganini and Sholto Douglas and Bernd Bohnet and Fabio Pardo and Seth Odoom and Mihaela Rosca and Cicero Nogueira dos Santos and Kedar Soparkar and Arthur Guez and Tom Hudson and Steven Hansen and Chulayuth Asawaroengchai and Ravi Addanki and Tianhe Yu and Wojciech Stokowiec and Mina Khan and Justin Gilmer and Jaehoon Lee and Carrie Grimes Bostock and Keran Rong and Jonathan Caton and Pedram Pejman and Filip Pavetic and Geoff Brown and Vivek Sharma and Mario Lučić and Rajkumar Samuel and Josip Djolonga and Amol Mandhane and Lars Lowe Sjösund and Elena Buchatskaya and Elspeth White and Natalie Clay and Jiepu Jiang and Hyeontaek Lim and Ross Hemsley and Zeyncep Cankara and Jane Labanowski and Nicola De Cao and David Steiner and Sayed Hadi Hashemi and Jacob Austin and Anita Gergely and Tim Blyth and Joe Stanton and Kaushik Shivakumar and Aditya Siddhant and Anders Andreassen and Carlos Araya and Nikhil Sethi and Rakesh Shivanna and Steven Hand and Ankur Bapna and Ali Khodaei and Antoine Miech and Garrett Tanzer and Andy Swing and Shantanu Thakoor and Lora Aroyo and Zhufeng Pan and Zachary Nado and Jakub Sygnowski and Stephanie Winkler and Dian Yu and Mohammad Saleh and Loren Maggiore and Yamini Bansal and Xavier Garcia and Mehran Kazemi and Piyush Patil and Ishita Dasgupta and Iain Barr and Minh Giang and Thais Kagohara and Ivo Danihelka and Amit Marathe and Vladimir Feinberg and Mohamed Elhawaty and Nimesh Ghelani and Dan Horgan and Helen Miller and Lexi Walker and Richard Tanburn and Mukarram Tariq and Disha Shrivastava and Fei Xia and Qingze Wang and Chung-Cheng Chiu and Zoe Ashwood and Khuslen Baatarsukh and Sina Samangooei and Raphaël Lopez Kaufman and Fred Alcober and Axel Stjerngren and Paul Komarek and Katerina Tsihlas and Anudhyan Boral and Ramona Comanescu and Jeremy Chen and Ruibo Liu and Chris Welty and Dawn Bloxwich and Charlie Chen and Yanhua Sun and Fangxiaoyu Feng and Matthew Mauger and Xerxes Dotiwalla and Vincent Hellendoorn and Michael Sharman and Ivy Zheng and Krishna Haridasan and Gabe Barth-Maron and Craig Swanson and Dominika Rogozińska and Alek Andreev and Paul Kishan Rubenstein and Ruoxin Sang and Dan Hurt and Gamaleldin Elsayed and Renshen Wang and Dave Lacey and Anastasija Ilić and Yao Zhao and Adam Iwanicki and Alejandro Lince and Alexander Chen and Christina Lyu and Carl Lebsack and Jordan Griffith and Meenu Gaba and Paramjit Sandhu and Phil Chen and Anna Koop and Ravi Rajwar and Soheil Hassas Yeganeh and Solomon Chang and Rui Zhu and Soroush Radpour and Elnaz Davoodi and Ving Ian Lei and Yang Xu and Daniel Toyama and Constant Segal and Martin Wicke and Hanzhao Lin and Anna Bulanova and Adrià Puigdomènech Badia and Nemanja Rakićević and Pablo Sprechmann and Angelos Filos and Shaobo Hou and Víctor Campos and Nora Kassner and Devendra Sachan and Meire Fortunato and Chimezie Iwuanyanwu and Vitaly Nikolaev and Balaji Lakshminarayanan and Sadegh Jazayeri and Mani Varadarajan and Chetan Tekur and Doug Fritz and Misha Khalman and David Reitter and Kingshuk Dasgupta and Shourya Sarcar and Tina Ornduff and Javier Snaider and Fantine Huot and Johnson Jia and Rupert Kemp and Nejc Trdin and Anitha Vijayakumar and Lucy Kim and Christof Angermueller and Li Lao and Tianqi Liu and Haibin Zhang and David Engel and Somer Greene and Anaïs White and Jessica Austin and Lilly Taylor and Shereen Ashraf and Dangyi Liu and Maria Georgaki and Irene Cai and Yana Kulizhskaya and Sonam Goenka and Brennan Saeta and Ying Xu and Christian Frank and Dario de Cesare and Brona Robenek and Harry Richardson and Mahmoud Alnahlawi and Christopher Yew and Priya Ponnapalli and Marco Tagliasacchi and Alex Korchemniy and Yelin Kim and Dinghua Li and Bill Rosgen and Kyle Levin and Jeremy Wiesner and Praseem Banzal and Praveen Srinivasan and Hongkun Yu and Çağlar Ünlü and David Reid and Zora Tung and Daniel Finchelstein and Ravin Kumar and Andre Elisseeff and Jin Huang and Ming Zhang and Ricardo Aguilar and Mai Giménez and Jiawei Xia and Olivier Dousse and Willi Gierke and Damion Yates and Komal Jalan and Lu Li and Eri Latorre-Chimoto and Duc Dung Nguyen and Ken Durden and Praveen Kallakuri and Yaxin Liu and Matthew Johnson and Tomy Tsai and Alice Talbert and Jasmine Liu and Alexander Neitz and Chen Elkind and Marco Selvi and Mimi Jasarevic and Livio Baldini Soares and Albert Cui and Pidong Wang and Alek Wenjiao Wang and Xinyu Ye and Krystal Kallarackal and Lucia Loher and Hoi Lam and Josef Broder and Dan Holtmann-Rice and Nina Martin and Bramandia Ramadhana and Mrinal Shukla and Sujoy Basu and Abhi Mohan and Nick Fernando and Noah Fiedel and Kim Paterson and Hui Li and Ankush Garg and Jane Park and DongHyun Choi and Diane Wu and Sankalp Singh and Zhishuai Zhang and Amir Globerson and Lily Yu and John Carpenter and Félix de Chaumont Quitry and Carey Radebaugh and Chu-Cheng Lin and Alex Tudor and Prakash Shroff and Drew Garmon and Dayou Du and Neera Vats and Han Lu and Shariq Iqbal and Alex Yakubovich and Nilesh Tripuraneni and James Manyika and Haroon Qureshi and Nan Hua and Christel Ngani and Maria Abi Raad and Hannah Forbes and Jeff Stanway and Mukund Sundararajan and Victor Ungureanu and Colton Bishop and Yunjie Li and Balaji Venkatraman and Bo Li and Chloe Thornton and Salvatore Scellato and Nishesh Gupta and Yicheng Wang and Ian Tenney and Xihui Wu and Ashish Shenoy and Gabriel Carvajal and Diana Gage Wright and Ben Bariach and Zhuyun Xiao and Peter Hawkins and Sid Dalmia and Clement Farabet and Pedro Valenzuela and Quan Yuan and Ananth Agarwal and Mia Chen and Wooyeol Kim and Brice Hulse and Nandita Dukkipati and Adam Paszke and Andrew Bolt and Kiam Choo and Jennifer Beattie and Jennifer Prendki and Harsha Vashisht and Rebeca Santamaria-Fernandez and Luis C. Cobo and Jarek Wilkiewicz and David Madras and Ali Elqursh and Grant Uy and Kevin Ramirez and Matt Harvey and Tyler Liechty and Heiga Zen and Jeff Seibert and Clara Huiyi Hu and Andrey Khorlin and Maigo Le and Asaf Aharoni and Megan Li and Lily Wang and Sandeep Kumar and Norman Casagrande and Jay Hoover and Dalia El Badawy and David Soergel and Denis Vnukov and Matt Miecnikowski and Jiri Simsa and Praveen Kumar and Thibault Sellam and Daniel Vlasic and Samira Daruki and Nir Shabat and John Zhang and Guolong Su and Jiageng Zhang and Jeremiah Liu and Yi Sun and Evan Palmer and Alireza Ghaffarkhah and Xi Xiong and Victor Cotruta and Michael Fink and Lucas Dixon and Ashwin Sreevatsa and Adrian Goedeckemeyer and Alek Dimitriev and Mohsen Jafari and Remi Crocker and Nicholas FitzGerald and Aviral Kumar and Sanjay Ghemawat and Ivan Philips and Frederick Liu and Yannie Liang and Rachel Sterneck and Alena Repina and Marcus Wu and Laura Knight and Marin Georgiev and Hyo Lee and Harry Askham and Abhishek Chakladar and Annie Louis and Carl Crous and Hardie Cate and Dessie Petrova and Michael Quinn and Denese Owusu-Afriyie and Achintya Singhal and Nan Wei and Solomon Kim and Damien Vincent and Milad Nasr and Christopher A. Choquette-Choo and Reiko Tojo and Shawn Lu and Diego de Las Casas and Yuchung Cheng and Tolga Bolukbasi and Katherine Lee and Saaber Fatehi and Rajagopal Ananthanarayanan and Miteyan Patel and Charbel Kaed and Jing Li and Shreyas Rammohan Belle and Zhe Chen and Jaclyn Konzelmann and Siim Põder and Roopal Garg and Vinod Koverkathu and Adam Brown and Chris Dyer and Rosanne Liu and Azade Nova and Jun Xu and Alanna Walton and Alicia Parrish and Mark Epstein and Sara McCarthy and Slav Petrov and Demis Hassabis and Koray Kavukcuoglu and Jeffrey Dean and Oriol Vinyals}
}



@misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wei2023chainofthought,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      eprint={2201.11903},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Peng_Xu_Bing_Huang_Lu_Si_2020, title={Knowing What, How and Why: A Near Complete Solution for Aspect-Based Sentiment Analysis}, volume={34}, url={https://ojs.aaai.org/index.php/AAAI/article/view/6383}, DOI={10.1609/aaai.v34i05.6383}, abstractNote={&lt;p&gt;Target-based sentiment analysis or aspect-based sentiment analysis (ABSA) refers to addressing various sentiment analysis tasks at a fine-grained level, which includes but is not limited to aspect extraction, aspect sentiment classification, and opinion extraction. There exist many solvers of the above individual subtasks or a combination of two subtasks, and they can work together to tell a complete story, i.e. the discussed aspect, the sentiment on it, and the cause of the sentiment. However, no previous ABSA research tried to provide a complete solution in one shot. In this paper, we introduce a new subtask under ABSA, named aspect sentiment triplet extraction (&lt;strong&gt;ASTE&lt;/strong&gt;). Particularly, a solver of this task needs to extract triplets (What, How, Why) from the inputs, which show WHAT the targeted aspects are, HOW their sentiment polarities are and WHY they have such polarities (i.e. opinion reasons). For instance, one triplet from “Waiters are very friendly and the pasta is simply average” could be (‘Waiters’, positive, ‘friendly’). We propose a two-stage framework to address this task. The first stage predicts what, how and why in a unified model, and then the second stage pairs up the predicted what (how) and why from the first stage to output triplets. In the experiments, our framework has set a benchmark performance in this novel triplet extraction task. Meanwhile, it outperforms a few strong baselines adapted from state-of-the-art related methods.&lt;/p&gt;}, number={05}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Peng, Haiyun and Xu, Lu and Bing, Lidong and Huang, Fei and Lu, Wei and Si, Luo}, year={2020}, month={Apr.}, pages={8600-8607} }


@article{Zhang2024,
  author    = {Hao Zhang and Yu-N Cheah and Osamah Mohammed Alyasiri and Jieyu An},
  title     = {Exploring aspect-based sentiment quadruple extraction with implicit aspects, opinions, and ChatGPT: a comprehensive survey},
  journal   = {Artificial Intelligence Review},
  year      = {2024},
  volume    = {57},
  number    = {2},
  pages     = {17},
  doi       = {10.1007/s10462-023-10633-x},
  url       = {https://doi.org/10.1007/s10462-023-10633-x},
  issn      = {1573-7462},
  abstract  = {In contrast to earlier ABSA studies primarily concentrating on individual sentiment components, recent research has ventured into more complex ABSA tasks encompassing multiple elements, including pair, triplet, and quadruple sentiment analysis. Quadruple sentiment analysis, also called aspect-category-opinion-sentiment quadruple Extraction (ACOSQE), aims to dissect aspect terms, aspect categories, opinion terms, and sentiment polarities while considering implicit sentiment within sentences. Nonetheless, a comprehensive overview of ACOSQE and its corresponding solutions is currently lacking. This is the precise gap that our survey seeks to address. To be more precise, we systematically reclassify all subtasks of ABSA, reorganizing existing research from the perspective of the involved sentiment elements, with a primary focus on the latest advancements in the ACOSQE task. Regarding solutions, our survey offers a comprehensive summary of the state-of-the-art utilization of language models within the ACOSQE task. Additionally, we explore the application of ChatGPT in sentiment analysis. Finally, we review emerging trends and discuss the challenges, providing insights into potential future directions for ACOSQE within the broader context of ABSA.},
}


@misc{wang2024negativeprompt,
      title={NegativePrompt: Leveraging Psychology for Large Language Models Enhancement via Negative Emotional Stimuli}, 
      author={Xu Wang and Cheng Li and Yi Chang and Jindong Wang and Yuan Wu},
      year={2024},
      eprint={2405.02814},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{Zhu2023ASO,
  title={A Survey on Model Compression for Large Language Models},
  author={Xunyu Zhu and Jian Li and Yong Liu and Can Ma and Weiping Wang},
  journal={ArXiv},
  year={2023},
  volume={abs/2308.07633},
  url={https://api.semanticscholar.org/CorpusID:260900101}
}


@inproceedings{peper-etal-2024-shoes,
    title = "Shoes-{ACOSI}: A Dataset for Aspect-Based Sentiment Analysis with Implicit Opinion Extraction",
    author = "Peper, Joseph J  and
      Qiu, Wenzhao  and
      Bruggeman, Ryan  and
      Han, Yi  and
      Chehade, Estefania Ciliotta  and
      Wang, Lu",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.907/",
    doi = "10.18653/v1/2024.findings-emnlp.907",
    pages = "15477--15490",
    abstract = "We explore *implicit opinion extraction* as a new component of aspect-based sentiment analysis (ABSA) systems. Prior work in ABSA has investigated opinion extraction as an important subtask, however, these works only label concise, *explicitly*-stated opinion spans. In this work, we present **Shoes-ACOSI**, a new and challenging ABSA dataset in the e-commerce domain with implicit opinion span annotations, the first of its kind. Shoes-ACOSI builds upon the existing Aspect-Category-Opinion-Sentiment (ACOS) quadruple extraction task, extending the task to quintuple extraction{---}now localizing and differentiating both implicit and explicit opinion. In addition to the new annotation schema, our dataset contains paragraph-length inputs which, importantly, present complex challenges through increased input length, increased number of sentiment expressions, and more mixed-sentiment-polarity examples when compared with existing benchmarks. We quantify the difficulty of our new dataset by evaluating with state-of-the-art fully-supervised and prompted-LLM baselines. We find our dataset presents significant challenges for both supervised models and LLMs, particularly from the new implicit opinion extraction component of the ACOSI task, highlighting the need for continued research into implicit opinion understanding."
}


@misc{qwen2025qwen25technicalreport,
      title={Qwen2.5 Technical Report}, 
      author={Qwen and : and An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tianyi Tang and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},
      year={2025},
      eprint={2412.15115},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.15115}, 
}


@misc{grattafiori2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Llama and : and Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Alex Vaughan and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurelien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Canton Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and Danny Wyatt and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Francisco Guzmán and Frank Zhang and Gabriel Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Govind Thattai and Graeme Nail and Gregoire Mialon and Guan Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel Kloumann and Ishan Misra and Ivan Evtimov and Jack Zhang and Jade Copet and Jaewon Lee and Jan Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and Jelmer van der Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and et al.},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}



@inproceedings{bai-etal-2024-compound,
    title = "Is Compound Aspect-Based Sentiment Analysis Addressed by {LLM}s?",
    author = "Bai, Yinhao  and
      Han, Zhixin  and
      Zhao, Yuhua  and
      Gao, Hang  and
      Zhang, Zhuowei  and
      Wang, Xunzhi  and
      Hu, Mengting",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.460/",
    doi = "10.18653/v1/2024.findings-emnlp.460",
    pages = "7836--7861",
    abstract = "Aspect-based sentiment analysis (ABSA) aims to predict aspect-based elements from the given text, mainly including four elements, i.e., aspect category, sentiment polarity, aspect term, and opinion term. Extracting pair, triple, or quad of elements is defined as compound ABSA. Due to its challenges and practical applications, such a compound scenario has become an emerging topic. Recently, large language models (LLMs), e.g. ChatGPT and LLaMA, present impressive abilities in tackling various human instructions. In this work, we are particularly curious whether LLMs still possess superior performance in handling compound ABSA tasks. To assess the performance of LLMs, we design a novel framework, called ChatABSA. Concretely, we design two strategies: constrained prompts, to automatically organize the returned predictions; post-processing, to better evaluate the capability of LLMs in recognition of implicit information. The overall evaluation involves 5 compound ABSA tasks and 8 publicly available datasets. We compare LLMs with few-shot supervised baselines and fully supervised baselines, including corresponding state-of-the-art (SOTA) models on each task. Experimental results show that ChatABSA exhibits excellent aspect-based sentiment analysis capabilities and overwhelmingly beats few-shot supervised methods under the same few-shot settings. Surprisingly, it can even outperform fully supervised methods in some cases. However, in most cases, it underperforms fully supervised methods, and there is still a huge gap between its performance and the SOTA method. Moreover, we also conduct more analyses to gain a deeper understanding of its sentiment analysis capabilities."
}


@article{spearman1904,
  author = {Charles Spearman},
  title = {The proof and measurement of association between two things},
  journal = {American Journal of Psychology},
  volume = {15},
  pages = {72--101},
  year = {1904}
}


@misc{promptstability,
      title={Prompt Stability Scoring for Text Annotation with Large Language Models}, 
      author={Christopher Barrie and Elli Palaiologou and Petter Törnberg},
      year={2025},
      eprint={2407.02039},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.02039}, 
}


@article{Hellwig2025DoWS,
  title={Do we still need Human Annotators? Prompting Large Language Models for Aspect Sentiment Quad Prediction},
  author={Nils Constantin Hellwig and Jakob Fehle and Udo Kruschwitz and Christian Wolff},
  journal={ArXiv},
  year={2025},
  volume={abs/2502.13044},
  url={https://api.semanticscholar.org/CorpusID:276421825}
}

@article{PING2024126994,
title = {Aspect category sentiment analysis based on prompt-based learning with attention mechanism},
journal = {Neurocomputing},
volume = {565},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126994},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223011177},
author = {Zhichao Ping and Guoming Sang and Zhi Liu and Yijia Zhang},
keywords = {Prompt-based learning, Aspect category sentiment analysis, Attention mechanism},
abstract = {Aspect category sentiment analysis (ACSA) excels at identifying the aspect categories and corresponding sentiments involved in a sentence, regardless of whether the aspect terms are explicitly mentioned or not. However, current methods tend to overinflate the original data, resulting in the introduction of unnecessary information, and fail to capture the inter-task relationship sufficiently. This paper presents a new method termed the prompt-based joint model (PBJM) to address these complications. PBJM treats the sentiment polarity prediction as binary classification and leverages a natural language prompt template, a concise sentence that guides the model to perform aspect category identification subtask and curtails the need for data augmentation. The two subtasks are jointly trained in pre-trained language models (PLMs) to capture their correlation. Further, the attention mechanism for aspect categories enables the model to concentrate selectively on significant features such as phrases and words during the predictions. In addition, the verbalizer employs a set of parameters to balance the weight of each label word while projecting between the label space and the label words space. Through experiments on four datasets, our model demonstrated remarkable performance in detecting category-sentiment pairs.}
}

@article{Xu2025,
  author = {Guixian Xu and Zhe Chen and Zixin Zhang},
  title = {Aspect category sentiment analysis based on pre-trained BiLSTM and syntax-aware graph attention network},
  journal = {Scientific Reports},
  year = {2025},
  volume = {15},
  doi = {10.1038/s41598-025-86009-8},
  url = {https://doi.org/10.1038/s41598-025-86009-8},
  issn = {2045-2322},
  abstract = {Aspect Category Sentiment Analysis (ACSA) is a fine-grained sentiment analysis task aimed at predicting the sentiment polarity associated with aspect categories within a sentence. Most existing ACSA methods are based on a given aspect category to locate sentiment words related to it. When irrelevant sentiment words have semantic meaning for the given aspect category, it may cause the problem that sentiment words cannot be matched with aspect categories. To address the aforementioned issue, this paper proposes a novel approach for ACSA utilizing pre-trained Bidirectional Long Short-Term Memory (BiLSTM) and syntax-aware graph attention network. To address the issue of insufficient existing annotated datasets, a method of using transfer learning is proposed. Firstly, the BiLSTM model is used to pre-train on the document-level sentiment analysis dataset, and the obtained pre-training parameters are transferred to the aspect-level task model. Then, a syntax-aware graph attention network model is proposed to make full use of the syntactic structure and semantic information in the text, and combine the knowledge learned in pre-training to achieve the ACSA task. The performance evaluation of this method is carried out on five user comment text datasets, and the comprehensive ablation experiments prove that this method performs best compared with baseline models.}
}


@misc{wei2025dontjustdemoteach,
      title={Don't Just Demo, Teach Me the Principles: A Principle-Based Multi-Agent Prompting Strategy for Text Classification}, 
      author={Peipei Wei and Dimitris Dimitriadis and Yan Xu and Mingwei Shen},
      year={2025},
      eprint={2502.07165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.07165}, 
}


@misc{li2024agentsneed,
      title={More Agents Is All You Need}, 
      author={Junyou Li and Qin Zhang and Yangbin Yu and Qiang Fu and Deheng Ye},
      year={2024},
      eprint={2402.05120},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.05120}, 
}


@misc{wang2024mixtureofagentsenhanceslargelanguage,
      title={Mixture-of-Agents Enhances Large Language Model Capabilities}, 
      author={Junlin Wang and Jue Wang and Ben Athiwaratkun and Ce Zhang and James Zou},
      year={2024},
      eprint={2406.04692},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.04692}, 
}


@misc{shorinwa2024surveyuncertaintyquantificationlarge,
      title={A Survey on Uncertainty Quantification of Large Language Models: Taxonomy, Open Research Challenges, and Future Directions}, 
      author={Ola Shorinwa and Zhiting Mei and Justin Lidard and Allen Z. Ren and Anirudha Majumdar},
      year={2024},
      eprint={2412.05563},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.05563}, 
}


@book{Pustejovsky_Stubbs_2013, place={Sebastopol, CA}, title={Natural language annotation for machine learning}, publisher={O’Reilly Media, Inc}, author={Pustejovsky, J. and Stubbs, Amber}, year={2013}} 


@inproceedings{pegasus,
author = {Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter J.},
title = {PEGASUS: pre-training with extracted gap-sentences for abstractive summarization},
year = {2020},
publisher = {JMLR.org},
abstract = {Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pretraining large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {1051},
numpages = {12},
series = {ICML 2020}
}


@misc{roberta,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1907.11692}, 
}


@article{scikit-learn,
author = {Pedregosa, Fabian and Varoquaux, Ga\"{e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, \'{E}douard},
title = {Scikit-learn: Machine Learning in Python},
year = {2011},
issue_date = {2/1/2011},
publisher = {JMLR.org},
volume = {12},
issn = {1532-4435},
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
journal = {J. Mach. Learn. Res.},
month = nov,
pages = {2825–2830},
numpages = {6}
}


@inproceedings{samuel-etal-2025-towards,
    title = "Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges",
    author = "Samuel, Vinay  and
      Zhou, Yue  and
      Zou, Henry Peng",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-main.338/",
    pages = "5058--5070",
    abstract = "As large language models achieve increasingly impressive results, questions arise about whether such performance is from generalizability or mere data memorization. Thus, numerous data contamination detection methods have been proposed. However, these approaches are often validated with traditional benchmarks and early-stage LLMs, leaving uncertainty about their effectiveness when evaluating state-of-the-art LLMs on the contamination of more challenging benchmarks. To address this gap and provide a dual investigation of SOTA LLM contamination status and detection method robustness, we evaluate five contamination detection approaches with four state-of-the-art LLMs across eight challenging datasets often used in modern LLM evaluation. Our analysis reveals that (1) Current methods have non-trivial limitations in their assumptions and practical applications; (2) Notable difficulties exist in detecting contamination introduced during instruction fine-tuning with answer augmentation; and (3) Limited consistencies between SOTA contamination detection techniques. These findings highlight the complexity of contamination detection in advanced LLMs and the urgent need for further research on robust and generalizable contamination evaluation."
}


@article{PAULLADA2021100336,
title = {Data and its (dis)contents: A survey of dataset development and use in machine learning research},
journal = {Patterns},
volume = {2},
number = {11},
pages = {100336},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100336},
url = {https://www.sciencedirect.com/science/article/pii/S2666389921001847},
author = {Amandalynne Paullada and Inioluwa Deborah Raji and Emily M. Bender and Emily Denton and Alex Hanna},
keywords = {datasets machine learning},
abstract = {Summary
In this work, we survey a breadth of literature that has revealed the limitations of predominant practices for dataset collection and use in the field of machine learning. We cover studies that critically review the design and development of datasets with a focus on negative societal impacts and poor outcomes for system performance. We also cover approaches to filtering and augmenting data and modeling techniques aimed at mitigating the impact of bias in datasets. Finally, we discuss works that have studied data practices, cultures, and disciplinary norms and discuss implications for the legal, ethical, and functional challenges the field continues to face. Based on these findings, we advocate for the use of both qualitative and quantitative approaches to more carefully document and analyze datasets during the creation and usage phases.}
}